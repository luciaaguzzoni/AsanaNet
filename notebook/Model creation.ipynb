{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75bfbb27",
   "metadata": {},
   "source": [
    "#### Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e16cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import precision_score\n",
    "#from sklearn.metrics import recall_score\n",
    "#from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Conv2D\n",
    "#from tensorflow.keras.layers import MaxPool2D\n",
    "#from tensorflow.keras.layers import Flatten\n",
    "#from tensorflow.keras.layers import Dropout\n",
    "#from tensorflow.keras.layers import Dense\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import pickle\n",
    "\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a643638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bdd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0666d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6226a65",
   "metadata": {},
   "source": [
    "#### Workflow:\n",
    "- Crear entorno nuevo con versión python 3.11\n",
    "- instalar tensorflow\n",
    "- Data Augmentation\n",
    "- aplicar una CNN\n",
    "- reserch de los parámetros y de la estructura: feed forward? capas? dense layers? loss functions? auc? batch? epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "(X_train,y_train) , (X_test,y_test)=mnist.load_data()\n",
    "\n",
    "#reshaping data\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1],X_test.shape[2],1))\n",
    "\n",
    "#checking the shape after reshaping\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#normalizing the pixel values\n",
    "X_train=X_train/255\n",
    "X_test=X_test/255\n",
    "\n",
    "#defining model\n",
    "model=Sequential()\n",
    "\n",
    "#adding convolution layer\n",
    "model.add(Conv2D(32,(3,3),activation=’relu’,input_shape=(28,28,1)))\n",
    "\n",
    "#adding pooling layer\n",
    "model.add(MaxPool2D(2,2))\n",
    "\n",
    "#adding fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100,activation=’relu’))\n",
    "\n",
    "#adding output layer\n",
    "model.add(Dense(10,activation=’softmax’))\n",
    "\n",
    "#compiling the model\n",
    "model.compile(loss=’sparse_categorical_crossentropy’,optimizer=’adam’,metrics=[‘accuracy’])\n",
    "\n",
    "#fitting the model\n",
    "model.fit(X_train,y_train,epochs=10)\n",
    "\n",
    "#evaluting the model\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define your CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "# Add more layers as needed\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation='softmax'))  # 100 output classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create an ImageDataGenerator for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load your dataset and split it into train, validation, and test sets\n",
    "#################### *\n",
    "\n",
    "# Train the model with data augmentation\n",
    "model.fit_generator(\n",
    "    datagen.flow(train_data, train_labels, batch_size=32),\n",
    "    steps_per_epoch=len(train_data) / 32,  # adjust batch_size and steps_per_epoch accordingly\n",
    "    epochs=50,\n",
    "    validation_data=(val_data, val_labels)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### *\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have a list of image paths and corresponding labels\n",
    "image_paths = [...]  # List of file paths to your images\n",
    "labels = [...]       # List of corresponding labels\n",
    "\n",
    "# Ensure that image_paths and labels are numpy arrays for easier manipulation\n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the dataset into training and temporary sets (combined validation and test)\n",
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_paths, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "Now, you have your datasets split into train, validation, and test sets\n",
    "train_paths, train_labels represent the training set\n",
    "val_paths, val_labels represent the validation set\n",
    "test_paths, test_labels represent the test set\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "In this example:\n",
    "train_test_split is used twice to create three sets: training, validation, and test sets.\n",
    "The test_size parameter determines the proportion of the dataset used for testing. In this case, 30% of the data is used for testing initially, and then half of that (15% of the total dataset) is allocated to the validation set.\n",
    "random_state is set for reproducibility, ensuring that the split is the same every time you run the code.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ed73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a05035f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a690f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0741b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dedee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ec3c044",
   "metadata": {},
   "source": [
    "#### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3804b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    # Function to load and preprocess images from a folder\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "            img = cv2.resize(img, (64, 64))  # Resize image ###########################\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folders_in_directory(directory_path):\n",
    "    # Get the list of all files and folders in the specified directory\n",
    "    items = os.listdir(directory_path)\n",
    "\n",
    "    # Filter out only the folders from the list\n",
    "    folders = [item for item in items if os.path.isdir(os.path.join(directory_path, item))]\n",
    "\n",
    "    return folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9846d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ebe73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19065a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88a3a599",
   "metadata": {},
   "source": [
    "#### 'main':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60154947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path of the directory you want to list folders from\n",
    "directory_path = '../data/dataset'\n",
    "\n",
    "# Call the function to get the list of folders\n",
    "asana = get_folders_in_directory(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7d231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63aaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and give labels\n",
    "X=[]\n",
    "y=[]\n",
    "for asana_name in asana:\n",
    "    path = directory_path + asana_name\n",
    "    X = X + load_images_from_folder(asana_name)\n",
    "    y.append(asana_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the image data\n",
    "X = X.reshape(X.shape[0], -1) # ??????????????????\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40) \n",
    "'''\n",
    "random_state : int, default=None. Controls the shuffling applied to the data before applying the split.\n",
    "                Pass an int for reproducible output across multiple function calls.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the following for different models and compare\n",
    "\n",
    "\n",
    "# Train logistic regression model\n",
    "'''\n",
    "ex:\n",
    "model = ()\n",
    "model.fit(X_train, y_train)\n",
    "'''\n",
    "\n",
    "# Evaluate model on test set\n",
    "'''\n",
    "ex:\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"F1 score: {f1_score(y_test, y_pred)}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa466cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7cf0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "\n",
    "'''\n",
    "model_file = '../models/asana_model.pkl'\n",
    "with open(model_file, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405676b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b94ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "model_file = '../models/logistic_cats_dogs.pkl' # pickle (pkl), .h5, ...\n",
    "with open(model_file, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f6322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551d3a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
